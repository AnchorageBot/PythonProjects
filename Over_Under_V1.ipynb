{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIPX8wBtitP3ononr4rZWZ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Kaggle - Model Underfitting and Overfitting\n",
        "\n",
        "*  We are going to [use the Melborne dataset](https://www.kaggle.com/code/dansbecker/model-validation/tutorial) to examine model underfitting and overfitting with this notebook\n",
        "* We are working through the [Kaggle Tutorial on underfitting and overfitting](https://www.kaggle.com/code/dansbecker/underfitting-and-overfitting)\n",
        "*  We are going to use the [Panda's library](https://pandas.pydata.org/docs/) Date: Dec 08, 2023 Version: 2.1.4\n",
        "*  We are also going to use the [Scikit library](https://scikit-learn.org/stable/) Date: October 2023.  Version: 1.3.2\n"
      ],
      "metadata": {
        "id": "2MskVYBqnvQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can drag and drop data files (csv files) that we want to work with from our local drive into the google colab file icon (left side of the colab screen)\n",
        "1.   Download the [Kaggle Melborne Housing Data](https://www.kaggle.com/code/dansbecker/model-validation/data) to your desktop\n",
        "2.   Click on the folder on left side of the approximate middle of the Colab screen\n",
        "3.   Drag and drop the melb_data.csv file into the folder to upload it to Google Colab from your desktop\n",
        "4.   You will need to do this operation everytime you use the notebook"
      ],
      "metadata": {
        "id": "i8bezjG2oTox"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our first script does the following:\n",
        "\n",
        "*   Imports the Panda's library\n",
        "*   Loads the data into Pandas"
      ],
      "metadata": {
        "id": "qDEwGIuhomM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Pandas Library\n",
        "import pandas as pd\n",
        "\n",
        "# Load data\n",
        "melbourne_data = pd.read_csv('melb_data.csv')"
      ],
      "metadata": {
        "id": "g4MF-kK4owRJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our next script does the following:\n",
        "\n",
        "*   Filters out dataset rows with missing data\n",
        "*   Identifies the target of the analysis and the features that impact it\n",
        "*   Imports the [scikit train test split library](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
        "*   Splits data into training and validation data, for both features and target\n",
        "\n"
      ],
      "metadata": {
        "id": "sRaxI3ZQ6FOO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "M_UUekxHnun2"
      },
      "outputs": [],
      "source": [
        "# Filter rows with missing price values\n",
        "filtered_melbourne_data = melbourne_data.dropna(axis=0)\n",
        "\n",
        "# Choose target and features\n",
        "y = filtered_melbourne_data.Price\n",
        "melbourne_features = ['Rooms',\n",
        "                      'Bathroom',\n",
        "                      'Landsize',\n",
        "                      'BuildingArea',\n",
        "                      'YearBuilt',\n",
        "                      'Lattitude',\n",
        "                      'Longtitude']\n",
        "X = filtered_melbourne_data[melbourne_features]\n",
        "\n",
        "# Import scikit library\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split data into training and validation data, for both features and target\n",
        "train_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will think about/look at why it's good to split up data sets into two parts:\n",
        "*  Training portion (model is familiar with this data)\n",
        "*  Validation portion (new data for the model and we can see how the model performs in the wild)\n",
        "\n",
        "We want a bias free model that can analyze datasets impartially"
      ],
      "metadata": {
        "id": "ITLg9IWZ9BT9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keeping this in mind, our next script accomplishes the following:\n",
        "*  Imports scikits libraries\n",
        "*  Uses a function to help compare MAE scores from different values for max_leaf_nodes\n",
        "\n",
        "It's helpful to review [scikit's DecisionTreeClassifier documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)"
      ],
      "metadata": {
        "id": "0u27QDzmZgEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import scikit libraries\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Function to help compare MAE scores from different values for max_leaf_nodes\n",
        "def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n",
        "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n",
        "    model.fit(train_X, train_y)\n",
        "    preds_val = model.predict(val_X)\n",
        "    mae = mean_absolute_error(val_y, preds_val)\n",
        "    return(mae)"
      ],
      "metadata": {
        "id": "WEUCrTyBYzdr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our next script:\n",
        "* Uses a for-loop to compare the accuracy of models built with different values for max_leaf_nodes."
      ],
      "metadata": {
        "id": "VPutTkKQbSzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compare MAE with differing values of max_leaf_nodes\n",
        "for max_leaf_nodes in [5, 50, 500, 5000]:\n",
        "    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n",
        "    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tK2k-Kbobexh",
        "outputId": "55de7654-0b02-4e20-a24a-a4ca11ccd3c3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max leaf nodes: 5  \t\t Mean Absolute Error:  347380\n",
            "Max leaf nodes: 50  \t\t Mean Absolute Error:  258171\n",
            "Max leaf nodes: 500  \t\t Mean Absolute Error:  243495\n",
            "Max leaf nodes: 5000  \t\t Mean Absolute Error:  255575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of the options listed, 500 is the optimal number of leaves to be used in the model\n",
        "\n",
        "Here's the takeaway: Models can suffer from either:\n",
        "\n",
        "* Overfitting: capturing spurious patterns that won't recur in the future, leading to less accurate predictions, or\n",
        "* Underfitting: failing to capture relevant patterns, again leading to less accurate predictions.\n",
        "\n",
        "We use validation data, which isn't used in model training, to measure a candidate model's accuracy. This lets us try many candidate models and keep the best one."
      ],
      "metadata": {
        "id": "0c7W8dTab2Tq"
      }
    }
  ]
}